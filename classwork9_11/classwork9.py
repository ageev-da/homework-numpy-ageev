# Конспект лекции 9
# ВЫПОЛНИЛ: АГЕЕВ ДАНИИЛ ЭДУАРДОВИЧ
# Машинное обучение

# ML - решает следующую задачу
# Требуется подогнать заднный набор точек данных под соответствующую функцию (отображение входа на выход),
# которая улавливает важные сигналы в данных и игнорирует помехи, а затем убедиться, что на новых данных функция работает хорошо.


# Обучение с учителем (supervised learning)
# Обучение без учителя (unsupervised learning)

# ОчУ - моделирует отношение между признаками и метками. Такие модели служат для предсказания меток на основе обучающих данных маркированных.
# После построения модели можно использовать её для присвоения меток новым ранее неизвестным данным 

# - задачи классификации (метки - дискретные: два или более)
# - задачи регрессии (метки/результат; непрерывные величины)

# ОбУ - моделирование признаки без меток. Такие модели служат для выявления структуры немаркированных данных

# - задача кластеризации (выделяет обдельные группы данных)
# - понижение размерности (поиск более сжатого представления данных)

# Существуют методы частичного обучения (semi-supervised learning). Не все данные промаркированы.

# Методы обучения с подкреплением (reinforcement learning). Система обучения улучшает свои характеристики на основе взаимодействия (обратной 
# связи) со средой. При этом взаимодействии система получает сигналы (функции наград), которые несут в себе информацию насколько хорошо/плохо
# система решила задачу (с точки зрения среды). Итоговая награда не станет максимальной.


import seaborn as sns

iris = sns.load_dataset('iris')

print(iris.head())
print(type(iris))
print(type(iris.values))
print(iris.values.shape)
print(iris.columns)
print(iris.index)


# Строки - отдельные объекты - образцы (sample)
# Столбцы - признаки (features) - соответствуют конкретным наблюдениям
# Матрицы признаков (features matrix) размер [число образцов x число признаков]
# Целевой массив, массив меток (targets) - одномерный массив [1 x число образцов] - данные, которые мы хотим предсказать на основе
# имеющихся данных
# Засивимые (метка) и независимые переменные (признаки)


# Процесс построения системы машинного обучения:

# 1. Предварительная обработка 
# - На вход поступают необработанные данные и метки
# - Происходит выбор признаков, масштабирование признаков
# - Понижение размерности 
# - Выборка образцов
# - На выход набор данных: обучающий, тестовый

# 2. Обучение
# - Выбор модели
# - Перекрестная проверка
# - Метрики эффективности
# - Оптимизация гиперпаарметров. Параметры, которые получаются не из данных, а являются характеристиками модели (настраиваемыми)

# 3. Оценка и формирование финальной модели

# 4. Прогнозирование (использование модели)



# !!! Будем использовать: SciKit-learn

# 1. Выбираем класс модели
# 2. Устанавливаем гиперпараметры модели
# 3. Создаем матрицу признаков и целевой массив
# 4. Обучение модели fit()
# 5. Применять модель к новым данным
# - predict() (с учителем)
# - predict() или transform() (обучение без учителя)


# Обучение с учителем: Линейная регрессия

## Простая линейная регрессия - аппроксимирует данные прямой линией

# y = ax + b, x - признаки, y - метки

import matplotlib.pyplot as plt
import numpy as np

# 1. Предварительная обработка - Получание данных (из датасета или генерация)
np.random.seed(1)
x = 10 * np.random.rand(50)

y = 2 * x - 1 + np.random.randn(50)
plt.scatter(x, y)
# 2. Обучение
# - Выбор модели
from sklearn.linear_model import LinearRegression
# - Устанавливаем гиперпараметры модели
model = LinearRegression(fit_intercept=True)
# - создаем матрицу признакв и целевой массив
print(x.shape)
print(y.shape)

X = x[:, np.newaxis]  # нужно второе измерение
# - обучение модели fit()
model.fit(X, y)

print(model.coef_[0]) # коэффициент b
print(model.intercept_) # коэффициент а

# Строим полученную прямую
x_ = np.linspace(0,10,30)
y_ = model.coef_[0] * x_ + model.intercept_

plt.plot(x_,y_)

# ПРИМЕНЕНЕИЕ МОДЕЛИ К НОВЫМ ДАННЫМ

xfit = np.linspace(0, 10, 5)
# xfit = np.linspace(-10, 10, 5) - может вообще непонятно что интерпретировать
yfit = model.predict(xfit[:, np.newaxis])

plt.scatter(xfit, yfit)
plt.show()





